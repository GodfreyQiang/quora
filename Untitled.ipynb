{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "from mxnet.contrib import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_quora():\n",
    "    train_data=pd.read_csv('./train/train.csv')\n",
    "    test_data=pd.read_csv('./test/test.csv')\n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data=read_quora()\n",
    "train_data = np.array(train_data).tolist()\n",
    "test_data = np.array(test_data).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000032939017120e6e44',\n",
       " 'Do you have an adopted dog, how would you encourage people to adopt and not shop?',\n",
       " 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0:][1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "对每个问题做分词，得到分好词的评论，观察问题类型发现，可以使用文本和标点符号分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_tokenized_quora(data):\n",
    "    def tokenizer(text):\n",
    "        return [tok.lower() for tok in re.split('\\?|; |, |\\*|\\n|\\s',text)]\n",
    "    return [tokenizer(question[1]) for question in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data[:int(len(train_data)/5*4)]\n",
    "valid_data=train_data[int(len(train_data)/5*4):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据分好词的训练数据创建词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_quora(data):\n",
    "    tokenized_data=get_tokenized_quora(data)\n",
    "    counter=collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return text.vocab.Vocabulary(counter,min_freq=3)\n",
    "vocab=get_vocab_quora(train_data+valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为每条问题长度不一致不能直接组合成小批量,定义预处理函数对每条评论进行分词，并通过词典转化为索引\n",
    "通过截断或补零将每个问题长度固定成100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90694"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import float32\n",
    "\n",
    "def preprocess_quora(data,vocab):\n",
    "    max_length=100\n",
    "    \n",
    "    def pad(x):\n",
    "        return x[:max_length] if len(x) >max_length else x + [float32(0)] * (max_length - len(x))\n",
    "    \n",
    "    tokenized_data=get_tokenized_quora(data)\n",
    "    features=np.array([pad(vocab.to_indices(x)) for x in tokenized_data],dtype=np.float32)\n",
    "    labels=np.array([item[2] for item in data],dtype=np.float32)\n",
    "#     print(features,labels)\n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import data as gdata, loss as gloss, nn, rnn, utils as gutils\n",
    "batch_size=64\n",
    "train_set=gdata.ArrayDataset(*preprocess_quora(train_data, vocab))\n",
    "valid_set = gdata.ArrayDataset(*preprocess_quora(valid_data, vocab))\n",
    "#test_set = gdata.ArrayDataset(*preprocess_quora(test_data, vocab))\n",
    "train_iter = gdata.DataLoader(train_set, batch_size, shuffle=True)\n",
    "valid_iter = gdata.DataLoader(valid_set, batch_size)\n",
    "#test_iter = gdata.DataLoader(test_set, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set = gdata.ArrayDataset(*preprocess_quora(test_data, vocab))\n",
    "# test_iter = gdata.DataLoader(test_set, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(mxnet.gluon.data.dataset.ArrayDataset, mxnet.gluon.data.dataloader.DataLoader)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_set),type(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (64, 100) y (64,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('#batches:', 16327)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for X, y in train_iter:\n",
    "    print('X', X.shape, 'y', y.shape)\n",
    "    break\n",
    "'#batches:', len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Block):\n",
    "    def __init__(self,vocab,embed_size,num_hiddens,num_layers,**kwargs):\n",
    "        super(BiRNN,self).__init__(**kwargs)\n",
    "        self.embedding=nn.Embedding(len(vocab),embed_size)\n",
    "        self.encoder=rnn.LSTM(num_hiddens,num_layers=num_layers,bidirectional=True,input_size=embed_size)\n",
    "        self.decoder=nn.Dense(2)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        embeddings=self.embedding(inputs.T)\n",
    "        states=self.encoder(embeddings)\n",
    "        encoding=nd.concat(states[0],states[-1])\n",
    "        outputs=self.decoder(encoding)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluonbook as gb\n",
    "from mxnet import gluon, init, nd\n",
    "embed_size,num_hiddens,num_layers,ctx=100,100,2,gb.try_all_gpus()\n",
    "net=BiRNN(vocab,embed_size,num_hiddens,num_layers)\n",
    "net.initialize(init.Xavier(),ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding=text.embedding.create('glove',pretrained_file_name='glove.6B.100d.txt',vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.embedding.weight.set_data(glove_embedding.idx_to_vec)\n",
    "net.embedding.collect_params().setattr('grad_req','null')\n",
    "#此处训练过程中词向量没有更新，是否能通过更新来做优化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on [cpu(0)]\n",
      "epoch 1, loss 0.1314, train acc 0.950, test acc 0.320, time 67217.6 sec\n",
      "epoch 2, loss 0.2502, train acc 0.938, test acc 0.938, time 82317.9 sec\n"
     ]
    }
   ],
   "source": [
    "import gluonbook as gb\n",
    "#学习率和迭代次数\n",
    "lr,num_epochs=0.01,5\n",
    "#创建trainer实例，指定学习率为0.01，使用adam优化算法用于迭代net实例所有通过add函数嵌套的层所包含的全部参数，这些参数通过collect_params获取\n",
    "trainer=gluon.Trainer(net.collect_params(),'adam',{'learning_rate':lr})\n",
    "#分开定义 softmax 运算和交叉熵损失函数可能会造成数值不稳定，这是一个包括softmax运算和交叉熵损失计算的函数，它的数值稳定性更好\n",
    "loss=gloss.SoftmaxCrossEntropyLoss()\n",
    "gb.train(train_iter,valid_iter,net,loss,trainer,ctx,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(net,vocab,sentence):\n",
    "    sentence=nd.array(vocab.to_indices(sentece),gb.try_gpu())\n",
    "    label=nd.argmax(net(sentence.reshape((1,-1))),axis=1)\n",
    "    return 1 if label.asscalar()==1 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(train_iter):\n",
    "    f,l=batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
